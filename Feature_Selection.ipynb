{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ffc62e9",
   "metadata": {},
   "source": [
    "### What is feature selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ab3ba2",
   "metadata": {},
   "source": [
    "Choosing the important features for the model is known as feature selection.Feature selection is a way of reducing the input variable for the model by using only relevant data in order to reduce overfitting in the model.Feature selection is performed by either including the important features or excluding the irrelevant features in the dataset without changing them.\n",
    "        Each machine learning process depends on feature engineering, which mainly contains two processes; which are Feature Selection and Feature Extraction.The main difference between them is that feature selection is about selecting the subset of the original feature set, whereas feature extraction creates new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606df67",
   "metadata": {},
   "source": [
    "\n",
    "Example:\n",
    "    Selecting the best features helps the model to perform well. For example, Suppose we want to create a model that automatically decides which Bike should be crushed for a spare part, and to do this, we have a dataset. This dataset contains a Model of the Bike, Year, Owner's name, Miles. So, in this dataset, the name of the owner does not contribute to the model performance as it does not decide if the Bike should be crushed or not, so we can remove this column and select the rest of the features(column) for the model building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58582b",
   "metadata": {},
   "source": [
    "###### Below are some benefits of using feature selection in machine learning:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad5f69",
   "metadata": {},
   "source": [
    "It helps in avoiding the curse of dimensionality.                                                  \n",
    "It helps in the simplification of the model so that it can be easily interpreted by the researchers.  \n",
    "It reduces the training time.                                                                         \n",
    "It reduces overfitting hence enhance the generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaeed32",
   "metadata": {},
   "source": [
    "### Some popular techniques of feature selection in machine learning are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9210f79",
   "metadata": {},
   "source": [
    "1.Filter methods                              \n",
    "2.Wrapper methods                        \n",
    "3.Embedded methods       \n",
    "\n",
    "Eg:        \n",
    "Embdded methods         \n",
    "Regularization – This method adds a penalty to different parameters of the machine learning model \n",
    "to avoid over-fitting of the model. This approach of feature selection uses (L1 regularization) and\n",
    "(L1 and L2 regularization). The penalty is applied over the coefficients, thus bringing down some coefficients to zero. The features having zero coefficient can be removed from the dataset.        \n",
    "Tree-based methods – These methods such as Random Forest, Gradient Boosting provides us feature \n",
    "importance as a way to select features as well. Feature importance tells us which features \n",
    "are more important in making an impact on the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e35c795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
